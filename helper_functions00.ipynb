{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "SAMPLE_AUDIO = '/work/dpandya/LibriVox_Kaggle/achtgesichterambiwasse/achtgesichterambiwasse_0007.wav'\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(path, resample=SAMPLE_RATE):\n",
    "    '''\n",
    "    Given a path to the audio file, returns a torch.Tensor array and sampling rate\n",
    "    \n",
    "    Args:\n",
    "    path: The path of the audio file\n",
    "    resample: The resampling rate, if different than default\n",
    "    '''\n",
    "    audio, sr = torchaudio.load(path)\n",
    "    \n",
    "    if (sr==resample):\n",
    "        return audio, sr\n",
    "    else:\n",
    "        resampler = T.Resample(sr, resample, dtype=audio.dtype)\n",
    "        audio = resampler(audio)\n",
    "        return audio, resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_length(audio):\n",
    "    '''\n",
    "    Returns the length of an audio in secs, given the sampling rate is 16000\n",
    "    '''\n",
    "    return len(audio[0])/SAMPLE_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_audio_chunks(audio, chunk_size=1):\n",
    "    '''\n",
    "    This funciton splits audio in chunks of n seconds. \n",
    "    You can adjust the chunk sizes by using chunks_size param\n",
    "\n",
    "    Args:\n",
    "    audio: torch.Tensor of shape [1,n_samples]\n",
    "    chunk_size: desired number of seconds in each chunk\n",
    "\n",
    "    Returns:\n",
    "    audio_chunks: returns a list of audio chunks of the predecided chunk length\n",
    "    '''\n",
    "    t_chunks = chunk_size*SAMPLE_RATE\n",
    "    audio_chunks = []\n",
    "    for i in range(0, len(audio[0]), t_chunks):\n",
    "        audio_chunks.append((audio[0][i:i+t_chunks]).unsqueeze(0))\n",
    "        \n",
    "    return audio_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyannote'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyannote\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[1;32m      2\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mpyannote/speaker-diarization@2.1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                     use_auth_token\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhf_vvWOjmbbsveKhMoDXhomItQAmcTcmVQHWx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# apply the pipeline to an audio file\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyannote'"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
    "                                    use_auth_token=\"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\")\n",
    "\n",
    "\n",
    "# apply the pipeline to an audio file\n",
    "diarization = pipeline(SAMPLE_AUDIO)\n",
    "\n",
    "# dump the diarization output to disk using RTTM format\n",
    "with open(\"audio.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "[tensor([[-1.1451e-04,  5.2144e-05, -1.0690e-04,  ...,  1.8235e-02,\n",
      "          1.9396e-02,  2.0040e-02]]), tensor([[ 0.0197,  0.0181,  0.0145,  ...,  0.0005, -0.0012, -0.0034]]), tensor([[ 0.0031,  0.0037, -0.0005,  ..., -0.0229,  0.0059,  0.0080]]), tensor([[-0.0174,  0.0030,  0.0041,  ..., -0.0371, -0.0422, -0.0455]]), tensor([[-0.0477, -0.0403, -0.0214,  ...,  0.0002,  0.0008,  0.0002]])]\n"
     ]
    }
   ],
   "source": [
    "aud, sr = get_samples(SAMPLE_AUDIO)\n",
    "\n",
    "ll = make_audio_chunks(aud, 2)\n",
    "print(sr)\n",
    "print([i for i in ll])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
