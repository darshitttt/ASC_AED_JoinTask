{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import panns_inference\n",
    "\n",
    "TUT_CSV = 'Datasets/TUT18_train.csv'\n",
    "SCAPPER_CSV = 'Datasets/scrapper_train_dataset.csv'\n",
    "TUT_AUD_DIR = '../audioData/TUTUrban2018/developmentDataset/TUT-urban-acoustic-scenes-2018-development/'\n",
    "SCAPPER_AUD_DIR = '../audioData/sythenticSoundscenes/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 Datasets/datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scapper_df = pd.read_csv(SCAPPER_CSV)\n",
    "scapper_scene_labels = (list)(scapper_df['acoustic_scene_label'].unique())\n",
    "\n",
    "def label_to_one_hot(labels, label_array=scapper_scene_labels):\n",
    "    \"\"\"\n",
    "    Convert string labels to one-hot encoded labels based on the provided array of labels.\n",
    "\n",
    "    Args:\n",
    "    - labels (list of str): List of string labels to convert.\n",
    "    - label_array (numpy array): Array containing all possible labels.\n",
    "\n",
    "    Returns:\n",
    "    - one_hot_encoded (numpy array): One-hot encoded labels corresponding to the input labels.\n",
    "    \"\"\"\n",
    "    label_dict = {label: i for i, label in enumerate(label_array)}\n",
    "    one_hot_encoded = np.zeros((len(labels), len(label_array)), dtype=int)\n",
    "    for i, label in enumerate(labels):\n",
    "        if label in label_dict:\n",
    "            one_hot_encoded[i, label_dict[label]] = 1\n",
    "    return torch.tensor(one_hot_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('Datasets/')\n",
    "sys.path.append('utils/')\n",
    "import datasets\n",
    "import audio_utils\n",
    "\n",
    "scapper_dataset = datasets.scraperDataset(SCAPPER_CSV, SCAPPER_AUD_DIR, only_scene=False, transforms=audio_utils.get_log_melSpectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160000])\n",
      "torch.Size([1, 160000])\n",
      "torch.Size([1, 160000])\n"
     ]
    }
   ],
   "source": [
    "tut_dataset = datasets.TUT18_Dataset(TUT_CSV, TUT_AUD_DIR, transforms=None)\n",
    "\n",
    "for i in range(3):\n",
    "    print(tut_dataset[i]['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 1501])\n",
      "torch.Size([1, 40, 1501])\n",
      "torch.Size([1, 40, 1501])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(scapper_dataset[i]['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 utils/audio_utils.py Datasets/datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(scapper_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(label_to_one_hot(batch['scene_label']))\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ASC_Model00(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ASC_Model00, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 2048)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Output layers\n",
    "        self.output_layer = nn.Linear(2048, 10)\n",
    "        self.softmax = nn.Softmax()\n",
    "        #self.second_last_layer = nn.Linear(2048, 2048)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        # Pooling layer\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten tensor\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        second_last = self.relu(self.fc3(x))\n",
    "        \n",
    "        # Output layers\n",
    "        output = self.output_layer(second_last)\n",
    "        #second_last = self.second_last_layer(x)\n",
    "        \n",
    "        return (output), second_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASC_Model00()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.286044120788574\n",
      "Epoch 2, Loss: 1.987437736480794\n",
      "Epoch 3, Loss: 1.8101707544732601\n",
      "Epoch 4, Loss: 1.6402381341508094\n",
      "Epoch 5, Loss: 1.5014876596471096\n",
      "Epoch 6, Loss: 1.2825214583823021\n",
      "Epoch 7, Loss: 1.0308161117929093\n",
      "Epoch 8, Loss: 0.8724342448280212\n",
      "Epoch 9, Loss: 0.7371335457614128\n",
      "Epoch 10, Loss: 0.5589560324207266\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = batch['spec'], label_to_one_hot(batch['scene_label'])\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device = device, dtype=torch.float)\n",
    "\n",
    "        outputs, second_last = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0246, 0.0101, 0.1303, 0.7482, 0.0101, 0.0275, 0.9643, 0.7399, 0.1653,\n",
      "         0.0674]])\n",
      "tensor([[-3.6797, -4.5863, -1.8986,  1.0888, -4.5824, -3.5652,  3.2959,  1.0453,\n",
      "         -1.6197, -2.6277]])\n",
      "restaurant\n"
     ]
    }
   ],
   "source": [
    "i = 1963\n",
    "sample = os.path.join(SCAPPER_AUD_DIR, scapper_df['audio_fileNames'][i])\n",
    "audio = audio_utils.load_audio_from_file(sample)\n",
    "spec = audio_utils.get_log_melSpectrogram(audio)\n",
    "\n",
    "model = model.cpu()\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    output, second_last = model(spec.unsqueeze(0))\n",
    "    print(torch.sigmoid(output))\n",
    "    print(output)\n",
    "\n",
    "print(scapper_df['acoustic_scene_label'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bus',\n",
       " 'busystreet',\n",
       " 'office',\n",
       " 'openairmarket',\n",
       " 'park',\n",
       " 'quietstreet',\n",
       " 'restaurant',\n",
       " 'supermarket',\n",
       " 'tube',\n",
       " 'tubestation']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scapper_scene_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0]]\n",
      "[[1 0 0 0 0 0 0 0 0 0]]\n",
      "[[1 0 0 0 0 0 0 0 0 0]]\n",
      "[[1 0 0 0 0 0 0 0 0 0]]\n",
      "[[1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(label_to_one_hot([scapper_dataset[i]['scene_label']]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noiseremoval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
